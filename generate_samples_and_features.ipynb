{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config[\"target_phrase\"] = 'hey jar_vis'\n",
    "config[\"target_phrase_as_written\"] = 'hey jarvis'       # used to generate adverserial phrases\n",
    "config[\"model_name\"] = 'hey_jarvis'\n",
    "config[\"custom_negative_phrases\"] = ['hey', 'jar', 'hey jar', 'hey Johnny', 'hey Jacky', 'Beavis']\n",
    "\n",
    "config[\"samples_output_dir\"] = 'generated_samples'\n",
    "config[\"features_output_dir\"] = 'training_features/hey_jarvis'\n",
    "config[\"rirs_dir\"] = 'BUT_ReverbDB_rel_19_06_RIR-Only'  # directory containing Room Imnpulse Response files\n",
    "config[\"rirs_glob\"] = \"**/RIR/*.wav\"                    # Glob to choose the appropriate wav files for RIR\n",
    "config[\"audioset_clips_dir\"] = 'audioset_16k'           # directory containing converted Audioset wav files\n",
    "config[\"fma_clips_dir\"] = 'fma'                         # directory containing converted FreeMusicArchive wav files\n",
    "config[\"fsd_clips_dir\"] = 'fsd'                         # directory containing converted FSD50K wav files\n",
    "\n",
    "config[\"n_samples\"] = 200000                            # number of training samples to generate\n",
    "config[\"n_samples_val\"] = 20000                         # number of testing and validation samples to generate\n",
    "config[\"tts_batch_size\"] = 20\n",
    "config[\"augment_batch_size\"] = 16\n",
    "config[\"clip_duration_ms\"] = 1430                       # generated clips longer than this are ignored when augmenting\n",
    "config[\"spectrogram_duration_ms\"] = 1490                # duration of the spectrogram (usually equivalent to clip_duration_ms + end_jitter_ms)\n",
    "config[\"sample_rate_hz\"] = 16000\n",
    "config[\"end_jitter_ms\"] = 60                            # augmented clips have up to this amount of blank noise at the end of the clip\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "config[\"samples_output_dir\"] = os.path.abspath(config[\"samples_output_dir\"])\n",
    "\n",
    "if not os.path.exists(config[\"samples_output_dir\"]):\n",
    "    os.mkdir(config[\"samples_output_dir\"])\n",
    "\n",
    "models_samples_directory = os.path.join(config[\"samples_output_dir\"], config[\"model_name\"])\n",
    "if not os.path.exists(models_samples_directory):\n",
    "    os.mkdir(models_samples_directory)\n",
    "\n",
    "positive_train_output_dir = os.path.join(models_samples_directory, \"positive_train\")\n",
    "positive_test_output_dir = os.path.join(models_samples_directory, \"positive_test\")\n",
    "positive_validation_output_dir = os.path.join(models_samples_directory, \"positive_validation\")\n",
    "negative_train_output_dir = os.path.join(models_samples_directory, \"negative_train\")\n",
    "negative_test_output_dir = os.path.join(models_samples_directory, \"negative_test\")\n",
    "negative_validation_output_dir = os.path.join(models_samples_directory, \"negative_validation\")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.lite.experimental.microfrontend.python.ops import audio_microfrontend_op as frontend_op \n",
    "\n",
    "def generate_features_for_clip(clip):\n",
    "    micro_frontend = frontend_op.audio_microfrontend(\n",
    "        tf.convert_to_tensor(clip),\n",
    "        sample_rate=16000,\n",
    "        window_size=30,\n",
    "        window_step=20,\n",
    "        num_channels=40,\n",
    "        upper_band_limit=7500,\n",
    "        lower_band_limit=125,\n",
    "        enable_pcan=True,\n",
    "        min_signal_remaining=0.05,\n",
    "        out_scale=1,\n",
    "        out_type=tf.float32)\n",
    "    output = tf.multiply(micro_frontend, 0.0390625)\n",
    "    return output.numpy()\n",
    "\n",
    "def features_generator(generator):\n",
    "    for data in generator:\n",
    "        for clip in data:\n",
    "            yield generate_features_for_clip(clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate positive and negative samples\n",
    "\n",
    "import os\n",
    "import uuid\n",
    "import torch\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "if \"piper-sample-generator/\" not in sys.path:\n",
    "    sys.path.append(\"piper-sample-generator/\")\n",
    "from generate_samples import generate_samples\n",
    "\n",
    "if \".openwakeword/openwakeword\" not in sys.path:\n",
    "    sys.path.append('./openwakeword/openwakeword')\n",
    "from data import generate_adversarial_texts\n",
    "\n",
    "# Generate positive clips for training\n",
    "if not os.path.exists(positive_train_output_dir):\n",
    "    os.mkdir(positive_train_output_dir)\n",
    "\n",
    "n_current_samples = len(os.listdir(positive_train_output_dir))\n",
    "if n_current_samples <= 0.95*config[\"n_samples\"]:\n",
    "    generate_samples(\n",
    "        text=config[\"target_phrase\"], max_samples=config[\"n_samples\"]-n_current_samples,\n",
    "        batch_size=config[\"tts_batch_size\"],\n",
    "        noise_scales=[0.98], noise_scale_ws=[0.98], length_scales=[0.75, 1.0, 1.25],\n",
    "        output_dir=positive_train_output_dir, auto_reduce_batch_size=True,\n",
    "        file_names=[uuid.uuid4().hex + \".wav\" for i in range(config[\"n_samples\"])]\n",
    "    )\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    logging.warning(f\"Skipping generation of positive clips for training, as ~{config['n_samples']} already exist\")\n",
    "\n",
    "# Generate positive clips for testing\n",
    "logging.info(\"#\"*50 + \"\\nGenerating positive clips for testing\\n\" + \"#\"*50)\n",
    "if not os.path.exists(positive_test_output_dir):\n",
    "    os.mkdir(positive_test_output_dir)\n",
    "n_current_samples = len(os.listdir(positive_test_output_dir))\n",
    "        \n",
    "if n_current_samples <= 0.95*config[\"n_samples_val\"]:\n",
    "    generate_samples(text=config[\"target_phrase\"], max_samples=config[\"n_samples_val\"]-n_current_samples,\n",
    "                     batch_size=config[\"tts_batch_size\"],\n",
    "                     noise_scales=[1.0], noise_scale_ws=[1.0], length_scales=[0.75, 1.0, 1.25],\n",
    "                     output_dir=positive_test_output_dir, auto_reduce_batch_size=True)\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    logging.warning(f\"Skipping generation of positive clips testing, as ~{config['n_samples_val']} already exist\")\n",
    "    \n",
    "# Generate positive clips for validation\n",
    "logging.info(\"#\"*50 + \"\\nGenerating positive clips for validation\\n\" + \"#\"*50)\n",
    "if not os.path.exists(positive_validation_output_dir):\n",
    "    os.mkdir(positive_validation_output_dir)\n",
    "n_current_samples = len(os.listdir(positive_validation_output_dir))\n",
    "        \n",
    "if n_current_samples <= 0.95*config[\"n_samples_val\"]:\n",
    "    generate_samples(text=config[\"target_phrase\"], max_samples=config[\"n_samples_val\"]-n_current_samples,\n",
    "                     batch_size=config[\"tts_batch_size\"],\n",
    "                     noise_scales=[1.0], noise_scale_ws=[1.0], length_scales=[0.75, 1.0, 1.25],\n",
    "                     output_dir=positive_validation_output_dir, auto_reduce_batch_size=True)\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    logging.warning(f\"Skipping generation of positive clips validation, as ~{config['n_samples_val']} already exist\")\n",
    "    \n",
    "# Generate adversarial negative clips for training\n",
    "logging.info(\"#\"*50 + \"\\nGenerating negative clips for training\\n\" + \"#\"*50)\n",
    "if not os.path.exists(negative_train_output_dir):\n",
    "    os.mkdir(negative_train_output_dir)\n",
    "n_current_samples = len(os.listdir(negative_train_output_dir))\n",
    "if n_current_samples <= 0.95*config[\"n_samples\"]:\n",
    "    adversarial_texts = config[\"custom_negative_phrases\"]\n",
    "    for target_phrase in config[\"target_phrase\"]:\n",
    "        adversarial_texts.extend(generate_adversarial_texts(\n",
    "            input_text=target_phrase,\n",
    "            N=config[\"n_samples\"]//len(config[\"target_phrase\"]),\n",
    "            include_partial_phrase=1.0,\n",
    "            include_input_words=0.2))\n",
    "    generate_samples(text=adversarial_texts, max_samples=config[\"n_samples\"]-n_current_samples,\n",
    "                     batch_size=config[\"tts_batch_size\"]//7,\n",
    "                     noise_scales=[0.98], noise_scale_ws=[0.98], length_scales=[0.75, 1.0, 1.25],\n",
    "                     output_dir=negative_train_output_dir, auto_reduce_batch_size=True,\n",
    "                     file_names=[uuid.uuid4().hex + \".wav\" for i in range(config[\"n_samples\"])]\n",
    "                     )\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    logging.warning(f\"Skipping generation of negative clips for training, as ~{config['n_samples']} already exist\")\n",
    "\n",
    "# Generate adversarial negative clips for testing\n",
    "logging.info(\"#\"*50 + \"\\nGenerating negative clips for testing\\n\" + \"#\"*50)\n",
    "if not os.path.exists(negative_test_output_dir):\n",
    "    os.mkdir(negative_test_output_dir)\n",
    "n_current_samples = len(os.listdir(negative_test_output_dir))\n",
    "if n_current_samples <= 0.95*config[\"n_samples_val\"]:\n",
    "    adversarial_texts = config[\"custom_negative_phrases\"]\n",
    "    for target_phrase in config[\"target_phrase\"]:\n",
    "        adversarial_texts.extend(generate_adversarial_texts(\n",
    "            input_text=target_phrase,\n",
    "            N=config[\"n_samples_val\"]//len(config[\"target_phrase\"]),\n",
    "            include_partial_phrase=1.0,\n",
    "            include_input_words=0.2))\n",
    "    generate_samples(text=adversarial_texts, max_samples=config[\"n_samples_val\"]-n_current_samples,\n",
    "                     batch_size=config[\"tts_batch_size\"]//7,\n",
    "                     noise_scales=[1.0], noise_scale_ws=[1.0], length_scales=[0.75, 1.0, 1.25],\n",
    "                     output_dir=negative_test_output_dir, auto_reduce_batch_size=True)\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    logging.warning(f\"Skipping generation of negative clips for testing, as ~{config['n_samples_val']} already exist\")\n",
    "\n",
    "# Generate adversarial negative clips for validation\n",
    "logging.info(\"#\"*50 + \"\\nGenerating negative clips for validation\\n\" + \"#\"*50)\n",
    "if not os.path.exists(negative_validation_output_dir):\n",
    "    os.mkdir(negative_validation_output_dir)\n",
    "n_current_samples = len(os.listdir(negative_validation_output_dir))\n",
    "if n_current_samples <= 0.95*config[\"n_samples_val\"]:\n",
    "    adversarial_texts = config[\"custom_negative_phrases\"]\n",
    "    for target_phrase in config[\"target_phrase\"]:\n",
    "        adversarial_texts.extend(generate_adversarial_texts(\n",
    "            input_text=target_phrase,\n",
    "            N=config[\"n_samples_val\"]//len(config[\"target_phrase\"]),\n",
    "            include_partial_phrase=1.0,\n",
    "            include_input_words=0.2))\n",
    "    generate_samples(text=adversarial_texts, max_samples=config[\"n_samples_val\"]-n_current_samples,\n",
    "                     batch_size=config[\"tts_batch_size\"]//7,\n",
    "                     noise_scales=[1.0], noise_scale_ws=[1.0], length_scales=[0.75, 1.0, 1.25],\n",
    "                     output_dir=negative_validation_output_dir, auto_reduce_batch_size=True)\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    logging.warning(f\"Skipping generation of negative clips for validation, as ~{config['n_samples_val']} already exist\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment generated samples by adding background noise and applying room impulse responses\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from mmap_ninja.ragged import RaggedMmap\n",
    "\n",
    "if \".openwakeword/openwakeword\" not in sys.path:\n",
    "    sys.path.append('./openwakeword/openwakeword')\n",
    "from data import augment_clips, filter_audio_paths\n",
    "\n",
    "config[\"rirs_dir\"] = os.path.abspath(config[\"rirs_dir\"])\n",
    "config[\"audioset_clips_dir\"] = os.path.abspath(config[\"audioset_clips_dir\"])\n",
    "config[\"fma_clips_dir\"] = os.path.abspath(config[\"fma_clips_dir\"])\n",
    "config[\"fsd_clips_dir\"] = os.path.abspath(config[\"fsd_clips_dir\"])\n",
    "config[\"features_output_dir\"] = os.path.abspath(config[\"features_output_dir\"])\n",
    "config[\"audio_samples_per_clip\"] = int((config[\"spectrogram_duration_ms\"])*config[\"sample_rate_hz\"]/1000) # ms * herz *1/(1000ms) = # of samples\n",
    "\n",
    "if not os.path.exists(config[\"features_output_dir\"]):\n",
    "    os.mkdir(config[\"features_output_dir\"])\n",
    "    \n",
    "max_duration_sec = config[\"clip_duration_ms\"]/1000.0\n",
    "spectrogram_duration_sec = config[\"spectrogram_duration_ms\"]/1000.0\n",
    "jitter_s = config[\"end_jitter_ms\"]/1000.0\n",
    "\n",
    "positive_clips_train, durations = filter_audio_paths([positive_train_output_dir],min_length_secs=0.0, max_length_secs=max_duration_sec, duration_method = \"size\")\n",
    "positive_clips_test, durations = filter_audio_paths([positive_test_output_dir],min_length_secs=0.0, max_length_secs=max_duration_sec, duration_method = \"size\")\n",
    "positive_clips_validation, durations = filter_audio_paths([positive_validation_output_dir],min_length_secs=0.0, max_length_secs=max_duration_sec, duration_method = \"size\")\n",
    "\n",
    "negative_clips_train, durations = filter_audio_paths([negative_train_output_dir],min_length_secs=0.0, max_length_secs=max_duration_sec, duration_method = \"size\")\n",
    "negative_clips_test, durations = filter_audio_paths([negative_test_output_dir],min_length_secs=0.0, max_length_secs=max_duration_sec, duration_method = \"size\")\n",
    "negative_clips_validation, durations = filter_audio_paths([negative_validation_output_dir],min_length_secs=0.0, max_length_secs=max_duration_sec, duration_method = \"size\")\n",
    "\n",
    "rir_paths = [str(i) for i in Path(config[\"rirs_dir\"]).glob(config[\"rirs_glob\"])]\n",
    "\n",
    "for i in range(0,1):\n",
    "    positive_train_generator = augment_clips(positive_clips_train,\n",
    "                        total_length=config[\"audio_samples_per_clip\"],\n",
    "                        batch_size=config[\"augment_batch_size\"],\n",
    "                        background_clip_paths=[config[\"audioset_clips_dir\"], config[\"fma_clips_dir\"], config[\"fsd_clips_dir\"]],\n",
    "                        RIR_paths=rir_paths, end_jitter=jitter_s)\n",
    "    positive_test_generator = augment_clips(positive_clips_test,\n",
    "                        total_length=config[\"audio_samples_per_clip\"],\n",
    "                        batch_size=config[\"augment_batch_size\"],\n",
    "                        background_clip_paths=[config[\"audioset_clips_dir\"], config[\"fma_clips_dir\"], config[\"fsd_clips_dir\"]],\n",
    "                        RIR_paths=rir_paths, end_jitter=jitter_s)\n",
    "    positive_validation_generator = augment_clips(positive_clips_validation,\n",
    "                        total_length=config[\"audio_samples_per_clip\"],\n",
    "                        batch_size=config[\"augment_batch_size\"],\n",
    "                        background_clip_paths=[config[\"audioset_clips_dir\"], config[\"fma_clips_dir\"], config[\"fsd_clips_dir\"]],\n",
    "                        RIR_paths=rir_paths, end_jitter=jitter_s)\n",
    "\n",
    "    negative_train_generator = augment_clips(negative_clips_train,\n",
    "                        total_length=config[\"audio_samples_per_clip\"],\n",
    "                        batch_size=config[\"augment_batch_size\"],\n",
    "                        background_clip_paths=[config[\"audioset_clips_dir\"], config[\"fma_clips_dir\"], config[\"fsd_clips_dir\"]],\n",
    "                        RIR_paths=rir_paths, end_jitter=jitter_s)\n",
    "    negative_test_generator = augment_clips(negative_clips_test,\n",
    "                        total_length=config[\"audio_samples_per_clip\"],\n",
    "                        batch_size=config[\"augment_batch_size\"],\n",
    "                        background_clip_paths=[config[\"audioset_clips_dir\"], config[\"fma_clips_dir\"], config[\"fsd_clips_dir\"]],\n",
    "                        RIR_paths=rir_paths, end_jitter=jitter_s)\n",
    "    negative_validation_generator = augment_clips(negative_clips_validation,\n",
    "                        total_length=config[\"audio_samples_per_clip\"],\n",
    "                        batch_size=config[\"augment_batch_size\"],\n",
    "                        background_clip_paths=[config[\"audioset_clips_dir\"], config[\"fma_clips_dir\"], config[\"fsd_clips_dir\"]],\n",
    "                        RIR_paths=rir_paths, end_jitter=jitter_s)\n",
    "\n",
    "    augmented_training_clips_directory = os.path.join(config[\"features_output_dir\"], \"training\")\n",
    "    augmented_testing_clips_directory = os.path.join(config[\"features_output_dir\"], \"testing\")\n",
    "    augmented_validation_clips_directory = os.path.join(config[\"features_output_dir\"], \"validation\")\n",
    "\n",
    "    if not os.path.exists(augmented_training_clips_directory):\n",
    "        os.mkdir(augmented_training_clips_directory)\n",
    "    if not os.path.exists(augmented_testing_clips_directory):\n",
    "        os.mkdir(augmented_testing_clips_directory)\n",
    "    if not os.path.exists(augmented_validation_clips_directory):\n",
    "        os.mkdir(augmented_validation_clips_directory)\n",
    "\n",
    "    augmented_positive_train_directory = os.path.join(augmented_training_clips_directory, \"wakeword\")\n",
    "    augmented_positive_test_directory = os.path.join(augmented_testing_clips_directory, \"wakeword\")\n",
    "    augmented_positive_validation_directory = os.path.join(augmented_validation_clips_directory, \"wakeword\")\n",
    "\n",
    "    augmented_negative_train_directory = os.path.join(augmented_training_clips_directory, \"unknown\")\n",
    "    augmented_negative_test_directory = os.path.join(augmented_testing_clips_directory, \"unknown\")\n",
    "    augmented_negative_validation_directory = os.path.join(augmented_validation_clips_directory, \"unknown\")\n",
    "\n",
    "    generator_outputs = [\n",
    "                         [positive_train_generator, augmented_positive_train_directory, len(positive_clips_train)], \n",
    "                         [positive_test_generator, augmented_positive_test_directory,len(positive_clips_test)],\n",
    "                         [positive_validation_generator, augmented_positive_validation_directory, len(positive_clips_validation)],\n",
    "                         [negative_train_generator, augmented_negative_train_directory, len(negative_clips_train)],\n",
    "                         [negative_test_generator, augmented_negative_test_directory, len(negative_clips_test)],\n",
    "                         [negative_validation_generator, augmented_negative_validation_directory, len(negative_clips_validation)]\n",
    "                        ]\n",
    "\n",
    "\n",
    "    for [generator, output_directory, n_total] in generator_outputs:\n",
    "        if not os.path.exists(output_directory):\n",
    "            os.mkdir(output_directory)\n",
    "\n",
    "        output_directory = os.path.join(output_directory, 'batch_'+str(i)+'_mmap')\n",
    "\n",
    "        RaggedMmap.from_generator(out_dir=output_directory,sample_generator=features_generator(generator),batch_size=1024,verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts a data set of samples to appropriately formatted wavs (for use as mixing background noises)\n",
    "# Could also use ffmpeg directly, which may be faster and can handle errors better\n",
    "# for clip in *.flac; do ffmpeg -i \"./$clip\" -hide_banner -loglevel error -sample_fmt s16 -ac 1 -ar 16000 \"./$clip.wav\"; done\n",
    "\n",
    "import datasets\n",
    "import scipy\n",
    "import os\n",
    "from pathlib import Path\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "path_to_audio_dataset = \"bal_train\"\n",
    "audio_dataset_glob = \"*.flac\"\n",
    "output_dir = 'audioset_16k'\n",
    "\n",
    "audioset_dataset = datasets.Dataset.from_dict({\"audio\": [str(i) for i in Path(path_to_audio_dataset).glob(audio_dataset_glob)]})\n",
    "audioset_dataset = audioset_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000))\n",
    "\n",
    "for row in tqdm(audioset_dataset):\n",
    "    name = row['audio']['path'].split('/')[-1].replace(\".flac\", \".wav\")\n",
    "    try:\n",
    "        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n",
    "    except Exception as e:\n",
    "        print(\"caught an issue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates features for background noise datasets\n",
    "# Input files do not need to be pre-converted to wav\n",
    "# Test and validations sets are truncated to the clip duration for consistent testing\n",
    "# Training sets convert the entire clip; the training process randomly truncates it each time used\n",
    "\n",
    "import datasets\n",
    "import sys\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from mmap_ninja.ragged import RaggedMmap\n",
    "\n",
    "if \".openwakeword/openwakeword\" not in sys.path:\n",
    "    sys.path.append('./openwakeword/openwakeword')\n",
    "\n",
    "from data import truncate_clip\n",
    "\n",
    "path_to_audio_dataset = \"BUT_ReverbDB_rel_19_06_RIR-Only\"\n",
    "audio_dataset_glob = \"**/silence/*.wav\"\n",
    "dataset_name = \"but_reverdb_silence\"\n",
    "\n",
    "audioset_dataset = datasets.Dataset.from_dict({\"audio\": [str(i) for i in Path(path_to_audio_dataset).glob(audio_dataset_glob)]})\n",
    "audioset_dataset = audioset_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000))\n",
    "\n",
    "train_testvalid = audioset_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "test_validate = train_testvalid['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "def features_generator(set):\n",
    "    if set == 'train':\n",
    "        for row in train_testvalid['train']:\n",
    "            if len(row['audio']['array']) < config[\"audio_samples_per_clip\"]:  # ensure clip has at least as many samples as needed\n",
    "                continue\n",
    "            yield generate_features_for_clip((row['audio']['array']*32767).astype(np.int16))\n",
    "    elif set == 'test':\n",
    "        for row in test_validate['test']:\n",
    "            if len(row['audio']['array']) < config[\"audio_samples_per_clip\"]:  # ensure clip has at least as many samples as needed\n",
    "                continue\n",
    "            \n",
    "            # Truncate for a consistent test set\n",
    "            truncated = truncate_clip((row['audio']['array']*32767).astype(np.int16), config[\"audio_samples_per_clip\"], \"random\")\n",
    "            yield generate_features_for_clip(truncated)\n",
    "    elif set == 'validate':\n",
    "        for row in test_validate['train']:\n",
    "            if len(row['audio']['array']) < config[\"audio_samples_per_clip\"]: # ensure clip has at least as many samples as needed\n",
    "                continue\n",
    "            \n",
    "            # Truncate for a consistent validation set\n",
    "            truncated = truncate_clip((row['audio']['array']*32767).astype(np.int16), config[\"audio_samples_per_clip\"], \"random\")\n",
    "            yield generate_features_for_clip(truncated)\n",
    "\n",
    "test_dir_fname = dataset_name + '_test_' + str(config[\"audio_samples_per_clip\"]) + 'ms_mmap'\n",
    "validation_dir_fname = dataset_name + '_validation_' + str(config[\"audio_samples_per_clip\"]) + 'ms_mmap'\n",
    "train_dir_fname = dataset_name + '_training_' + '_mmap'\n",
    "\n",
    "test_output_dir = os.join.path(augmented_negative_test_directory, test_dir_fname)\n",
    "validation_output_dir = os.join.path(augmented_negative_validation_directory, validation_dir_fname)\n",
    "train_output_dir = os.join.path(augmented_negative_train_directory, train_dir_fname)\n",
    "\n",
    "RaggedMmap.from_generator(out_dir=test_output_dir,\n",
    "                          sample_generator=features_generator('test'),\n",
    "                          batch_size=1024,\n",
    "                          verbose=True)\n",
    "RaggedMmap.from_generator(out_dir=validation_output_dir,\n",
    "                          sample_generator=features_generator('validate'), \n",
    "                          batch_size=1024,\n",
    "                          verbose=True)\n",
    "RaggedMmap.from_generator(out_dir=train_output_dir,\n",
    "                          sample_generator=features_generator('train'),\n",
    "                          batch_size=1024,\n",
    "                          verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all the required packages (borrowed from openWakeWord's automatic training notebook)\n",
    "\n",
    "!pip install torch\n",
    "!pip install torchaudio\n",
    "!pip install datasetsDD\n",
    "!pip install scipy\n",
    "!pip install tqdm\n",
    "!pip install jupyter\n",
    "!pip install ipywidgets\n",
    "!pip install mutagen\n",
    "!pip install torchinfo\n",
    "!pip install torchmetrics\n",
    "!pip install speechbrain\n",
    "!pip install audiomentations\n",
    "!pip install torch-audiomentations\n",
    "!pip install acoustics\n",
    "!pip install pronouncing\n",
    "!pip install datasets\n",
    "!pip install deep-phonemizer\n",
    "!pip install piper-phonemize\n",
    "!pip install webrtcvad\n",
    "!pip install datasets\n",
    "\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\"./piper-sample-generator\"):\n",
    "    !git clone https://github.com/rhasspy/piper-sample-generator\n",
    "    !wget -O piper-sample-generator/models/en_US-libritts_r-medium.pt 'https://github.com/rhasspy/piper-sample-generator/releases/download/v2.0.0/en_US-libritts_r-medium.pt'\n",
    "\n",
    "# install openwakeword (full installation to support training)\n",
    "!git clone https://github.com/dscripka/openwakeword\n",
    "!pip install -e ./openwakeword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloads a small collection of background noise and negative samples (borrowed from openWakeWord's automatic training notebook)\n",
    "\n",
    "import locale\n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\"\n",
    "locale.getpreferredencoding = getpreferredencoding\n",
    "\n",
    "# Download required models (workaround for Colab)\n",
    "import os\n",
    "\n",
    "# Imports\n",
    "import sys\n",
    "\n",
    "if \"piper-sample-generator/\" not in sys.path:\n",
    "    sys.path.append(\"piper-sample-generator/\")\n",
    "from generate_samples import generate_samples\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "import yaml\n",
    "import datasets\n",
    "import scipy\n",
    "from tqdm import tqdm\n",
    "\n",
    "## Download all data\n",
    "\n",
    "## Download MIR RIR data (takes about ~2 minutes)\n",
    "output_dir = \"./mit_rirs\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    rir_dataset = datasets.load_dataset(\"davidscripka/MIT_environmental_impulse_responses\", split=\"train\", streaming=True)\n",
    "    # Save clips to 16-bit PCM wav files\n",
    "    for row in tqdm(rir_dataset):\n",
    "        name = row['audio']['path'].split('/')[-1]\n",
    "        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n",
    "\n",
    "## Download noise and background audio (takes about ~3 minutes)\n",
    "\n",
    "# Audioset Dataset (https://research.google.com/audioset/dataset/index.html)\n",
    "# Download one part of the audioset .tar files, extract, and convert to 16khz\n",
    "# For full-scale training, it's recommended to download the entire dataset from\n",
    "# https://huggingface.co/datasets/agkphysics/AudioSet, and\n",
    "# even potentially combine it with other background noise datasets (e.g., FSD50k, Freesound, etc.)\n",
    "\n",
    "if not os.path.exists(\"audioset\"):\n",
    "    os.mkdir(\"audioset\")\n",
    "\n",
    "    fname = \"bal_train09.tar\"\n",
    "    out_dir = f\"audioset/{fname}\"\n",
    "    link = \"https://huggingface.co/datasets/agkphysics/AudioSet/resolve/main/\" + fname\n",
    "    !wget -O {out_dir} {link}\n",
    "    !cd audioset && tar -xvf bal_train09.tar\n",
    "\n",
    "    output_dir = \"./audioset_16k\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "\n",
    "    # Save clips to 16-bit PCM wav files\n",
    "    audioset_dataset = datasets.Dataset.from_dict({\"audio\": [str(i) for i in Path(\"audioset/audio\").glob(\"**/*.flac\")]})\n",
    "    audioset_dataset = audioset_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000))\n",
    "    for row in tqdm(audioset_dataset):\n",
    "        name = row['audio']['path'].split('/')[-1].replace(\".flac\", \".wav\")\n",
    "        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n",
    "\n",
    "# Free Music Archive dataset\n",
    "# https://github.com/mdeff/fma\n",
    "\n",
    "output_dir = \"./fma\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    fma_dataset = datasets.load_dataset(\"rudraml/fma\", name=\"small\", split=\"train\", streaming=True)\n",
    "    fma_dataset = iter(fma_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000)))\n",
    "\n",
    "    # Save clips to 16-bit PCM wav files\n",
    "    n_hours = 1  # use only 1 hour of clips for this example notebook, recommend increasing for full-scale training\n",
    "    for i in tqdm(range(n_hours*3600//30)):  # this works because the FMA dataset is all 30 second clips\n",
    "        row = next(fma_dataset)\n",
    "        name = row['audio']['path'].split('/')[-1].replace(\".mp3\", \".wav\")\n",
    "        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n",
    "        i += 1\n",
    "        if i == n_hours*3600//30:\n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
